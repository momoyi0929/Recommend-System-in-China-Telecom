import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import joblib
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset, ConcatDataset
from model import TwoTowerModel

# **è®¾å¤‡é€‰æ‹©**
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

# **1ï¸âƒ£ è¯»å–æ•°æ®**
user_df = pd.read_csv("/Users/momoyi0929/Desktop/æµ‹è¯•æ•°æ®3/user.csv")
item_df = pd.read_csv("/Users/momoyi0929/Desktop/æµ‹è¯•æ•°æ®3/item.csv")
interaction_df = pd.read_csv("/Users/momoyi0929/Desktop/æµ‹è¯•æ•°æ®3/label.csv")

user_df = user_df.drop_duplicates(subset=["phone"]).reset_index(drop=True)
item_df = item_df.drop_duplicates(subset=["äº§å“ID"]).reset_index(drop=True)
print("âœ… æ•°æ®è¯»å–å®Œæ¯•ï¼")

# **2ï¸âƒ£ é¢„å¤„ç†ç±»åˆ«ç‰¹å¾**
user_categ_features = ["pd_inst_id"]
item_categ_features = ["ä¸šåŠ¡", "åˆ†ç±»1", "æ”¯ä»˜æ–¹å¼", "äº§å“æœ‰æ•ˆæœŸ", "æ˜¯å¦æœ‰é™„å±æƒç›Š", "æ˜¯å¦æœ‰åè®®æœŸ", "æ”¶è´¹ç­–ç•¥", "é€€è®¢è§„åˆ™"]

encoders = {}
for col in user_categ_features:
    encoder = LabelEncoder()
    user_df[col] = encoder.fit_transform(user_df[col])
    encoders[col] = encoder

for col in item_categ_features:
    encoder = LabelEncoder()
    item_df[col] = encoder.fit_transform(item_df[col])
    encoders[col] = encoder

# **3ï¸âƒ£ é¢„å¤„ç†æ•°å€¼ç‰¹å¾**
user_num_features = [
    "po_type", "city_nm", "gender", "cust_tp", "age_f",
    "main_limit_mon_chrg_lvl", "ob_f", "data_flow_f", "date_flow_use_pr", "qy_by", "qy_dc",
    "short_video_f", "game_f", "webcast_f", "music_f", "entertainment_f", "local_life_f",
    "webclass_f", "parenting_education_f", "employee_f", "trip_f", "catering_f",
    "read_and_listen_f", "ebusiness_f", "payment_f", "txvideo_f", "youku_f", "bilibili_f",
    "mango_f", "txsyjsq_f", "iqiyi_f", "wangzhe_f", "bdwp_f", "keep_f", "meituan_f",
    "zyb_f", "tyyp_f", "xunlei_f", "zhihu_f", "xmly_kid_f", "yax_f", "didi_f", "gddt_f",
    "mtwm_f", "mcdonold_f", "kfc_f", "luckin_f", "naixue_f", "xicha_f", "chayan_f",
    "mixue_f", "qqyuedu_f", "zhangyue_f", "xmly_f", "ksjgs_f", "qmxs_f", "jd_f", "yzf_f",
    "didacx_f"
]
item_num_features = ["å®šä»·ç­–ç•¥ï¼ˆå…ƒï¼‰"]

user_scaler = MinMaxScaler()
item_scaler = MinMaxScaler()

user_df[user_num_features] = user_scaler.fit_transform(user_df[user_num_features])
item_df[item_num_features] = item_scaler.fit_transform(item_df[item_num_features])


# **4ï¸âƒ£ å¤„ç† ID**
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

user_df["phone"] = user_encoder.fit_transform(user_df["phone"])
item_df["äº§å“ID"] = item_encoder.fit_transform(item_df["äº§å“ID"])

interaction_df["phone"] = user_encoder.transform(interaction_df["phone"])
interaction_df["goods_id"] = item_encoder.transform(interaction_df["goods_id"])

print("âœ… æ•°æ®å¤„ç†å®Œæ¯•ï¼")

# ä¿å­˜ç¼–ç å™¨
joblib.dump(encoders, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/encoders.pkl")

joblib.dump(user_scaler, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/user_scaler.pkl")
joblib.dump(item_scaler, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/item_scaler.pkl")

joblib.dump(user_encoder, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/user_id_encoder.pkl")
joblib.dump(item_encoder, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/item_id_encoder.pkl")

print("âœ… ç¼–ç å™¨ä»¥ä¿å­˜ï¼")

# **5ï¸âƒ£ åˆå¹¶æ•°æ®**
merged_df = interaction_df.merge(user_df, on="phone").merge(item_df, left_on="goods_id", right_on="äº§å“ID")
final_features = ["phone", "goods_id"] + user_num_features + item_num_features + user_categ_features + item_categ_features
train_df, test_df = train_test_split(merged_df[final_features + ["label"]], test_size=0.2, random_state=42)

# **6ï¸âƒ£ è·å–ç±»åˆ«ç‰¹å¾çš„å”¯ä¸€å€¼æ•°é‡**
user_categ_sizes = [user_df[col].nunique() for col in user_categ_features]
item_categ_sizes = [item_df[col].nunique() for col in item_categ_features]

# **7ï¸âƒ£ è½¬æ¢ä¸º Tensor**
def to_tensor(df):
    return {
        "user_id": torch.tensor(df["phone"].values, dtype=torch.long),
        "item_id": torch.tensor(df["goods_id"].values, dtype=torch.long),
        "user_features": torch.tensor(df[user_num_features].values, dtype=torch.float32),
        "item_features": torch.tensor(df[item_num_features].values, dtype=torch.float32),
        "user_categ": torch.tensor(df[user_categ_features].values, dtype=torch.long),
        "item_categ": torch.tensor(df[item_categ_features].values, dtype=torch.long),
        "label": torch.tensor(df["label"].values, dtype=torch.float32)
    }

train_data = to_tensor(train_df)
test_data = to_tensor(test_df)


# **8ï¸âƒ£ åˆ›å»º DataLoader**
batch_size = 4096
train_dataset = TensorDataset(train_data["user_id"], train_data["item_id"],
                              train_data["user_features"], train_data["user_categ"],
                              train_data["item_features"], train_data["item_categ"], train_data["label"])

test_dataset = TensorDataset(test_data["user_id"], test_data["item_id"],
                             test_data["user_features"], test_data["user_categ"],
                             test_data["item_features"], test_data["item_categ"], test_data["label"])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)

torch.save(test_loader, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/test_data.pt")
torch.save(train_loader, "/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/train_data.pt")
print("âœ… åˆ›å»ºä¿å­˜ DataLoaderï¼")

# **9ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹**
embedding_dim = 512
model = TwoTowerModel(len(user_num_features), len(item_num_features),
                      user_categ_sizes, item_categ_sizes, embedding_dim).to(device)

# criterion = nn.MSELoss()
criterion = nn.BCELoss()
# criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
print("âœ… æ¨¡å‹åˆå§‹åŒ–ï¼")

# **ğŸ”Ÿ è®­ç»ƒæ¨¡å‹**
num_epochs = 40
print("ğŸš€ å¼€å§‹è®­ç»ƒï¼")
best_auc = 0

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    i = 0
    j = 0
    for user_id, item_id, user_features, user_categ, item_features, item_categ, label in test_loader:
        # **æ•°æ®ç§»åŠ¨åˆ° MPS/CPU**
        user_id, item_id = user_id.to(device), item_id.to(device)
        user_features, user_categ = user_features.to(device), user_categ.to(device)
        item_features, item_categ, label = item_features.to(device), item_categ.to(device), label.to(device)

        optimizer.zero_grad()
        outputs = model(user_id, item_id, user_features, user_categ, item_features, item_categ)
        label = label.view(-1, 1)  # è°ƒæ•´æ ‡ç­¾å½¢çŠ¶
        loss = criterion(outputs, label)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        i += 1

    # **11ï¸âƒ£ éªŒè¯æ¨¡å‹**
    model.eval()
    total_test_loss = 0
    all_labels, all_preds = [], []

    with torch.no_grad():
        for user_id, item_id, user_features, user_categ, item_features, item_categ, label in test_loader:
            user_id, item_id = user_id.to(device), item_id.to(device)
            user_features, user_categ = user_features.to(device), user_categ.to(device)
            item_features, item_categ, label = item_features.to(device), item_categ.to(device), label.to(device)

            test_outputs = model(user_id, item_id, user_features, user_categ, item_features, item_categ)
            label = label.view(-1, 1)  # è°ƒæ•´æ ‡ç­¾å½¢çŠ¶
            test_loss = criterion(test_outputs, label)
            total_test_loss += test_loss.item()
            j += 1

            all_labels.extend(label.cpu().numpy())
            all_preds.extend(test_outputs.cpu().numpy())


    # **è®¡ç®— AUC**
    auc = roc_auc_score(all_labels, all_preds)

    # **12ï¸âƒ£ ä¿å­˜æœ€ä½³æ¨¡å‹**
    if auc > best_auc:
        best_auc = auc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': total_loss,
        }, '/Users/momoyi0929/Desktop/æ¨¡å‹æ•°æ®3/best_model.pth')
        print(f"âœ… ç¬¬ {epoch+1} è½®ï¼šæ¨¡å‹ä¿å­˜ï¼ŒAUC = {auc:.4f}")

    print(f"âœ… Epoch {epoch+1} | Train Loss: {total_loss/i:.4f} | Test Loss: {total_test_loss/j:.4f} | Test AUC: {auc:.4f}")

print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
